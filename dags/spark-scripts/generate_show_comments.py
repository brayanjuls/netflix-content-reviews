import sys
from pyspark.sql import SparkSession
import pyspark.sql.types as tp
import random
import string
import datetime as dt

class ShowComments:

    def __init__(self,catalog_path,comments_destination_path):
        self.catalog_path=catalog_path
        self.comments_destination_path=comments_destination_path

    def randomString(self,stringLength=8):
        letters = string.ascii_lowercase
        return ''.join(random.choice(letters) for i in range(stringLength))


    def random_date(self,start, end):
        """
        This function will return a random datetime between two datetime
        objects.
        """
        delta = end - start
        int_delta = (delta.days * 24 * 60 * 60) + delta.seconds
        random_second = random.randrange(int_delta)
        return start + dt.timedelta(seconds=random_second)


    def getRedditDataFrameSchema(self):
        return tp.StructType([tp.StructField('show_title', tp.StringType(), True),
                              tp.StructField('show_director', tp.StringType(), True),
                              tp.StructField('submission_id', tp.StringType(), True),
                              tp.StructField('source', tp.StringType(), True),
                              tp.StructField('title', tp.StringType(), True),
                              tp.StructField('description', tp.StringType(), True),
                              tp.StructField('created_utc', tp.TimestampType(), True),
                              tp.StructField('author', tp.StringType(), True),
                              tp.StructField('score', tp.IntegerType(), True),
                              tp.StructField('spoiler', tp.BooleanType(), True),
                              tp.StructField('is_original_content', tp.BooleanType(), True),
                              tp.StructField('distinguished', tp.StringType(), True),
                              tp.StructField('link', tp.StringType(), True),
                              tp.StructField('comments', tp.ArrayType(tp.StructType([
                                  tp.StructField('comment_id', tp.StringType(), True),
                                  tp.StructField('body', tp.StringType(), True),
                                  tp.StructField('created_utc', tp.TimestampType(), True),
                                  tp.StructField('score', tp.IntegerType(), True),
                                  tp.StructField('parent_id', tp.StringType(), True),
                                  tp.StructField('submission_id', tp.StringType(), True)]
                              )), True)
                              ])    


    def generate_comments(self,shows,logger):
        content_rows=[]
        for show in shows.collect():
            index = 1
            title_split=show.title.split(":",1)
            content_title=title_split[0]
            row_comments = []
            cur_date = dt.datetime.now()
            for source in ['netflix','tvshows']:
                logger.info("comments for movie {} and soure {} are being generated".format(show.title,source))
                score = random.randint(0,100)
                for index in range(0,250):
                    comment = self.randomString(random.randint(8,50))
                    
                    created_date = self.random_date(cur_date - dt.timedelta(days=120),cur_date)
                    row_comments.append((str(index),comment,created_date,score,
                                               'default_parent','default_link')) 
                current_sm=(show.title, show.director, index,source, show.title, 'selftext',
                            cur_date,'default_author',
                              score,False,False,'','',row_comments)
                content_rows.append(current_sm) 
                index+=1
        return content_rows    


    def execute(self):
        spark = SparkSession.builder.appName("AutoGeneratedRedditCommentToGCS").getOrCreate()
        logger = spark._jvm.org.apache.log4j.Logger
        mylogger = logger.getLogger("AutoGeneratedRedditComments")

        shows = spark.read.parquet(self.catalog_path)
        mylogger.info('show data read')
        generated_show_comments = self.generate_comments(shows,mylogger)    
        generated_show_comments_df=spark.createDataFrame(generated_show_comments,self.getRedditDataFrameSchema())
        generated_show_comments_df.write.partitionBy(["show_title","show_director"]).parquet(self.comments_destination_path, mode="append")




catalog_path = sys.argv[1]
comments_destination_path = sys.argv[2]
show_comments=ShowComments(catalog_path,comments_destination_path)
show_comments.execute()
                